{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Information from Text\n",
    "\n",
    "## Information Extraction\n",
    "\n",
    "Information can be extracted from structured or unstructured data. In structured data, there is a predictable organization of entities and relationships. \n",
    "\n",
    "Let's a look at an example of extracting information related to companies and their locations, if the data was stored in Python as a list of tuples `(entity, relation, entity)`. Using this information, we'll try to answer the question \"Which organizations operate in Atlanta?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = [('Omnicom', 'IN', 'New York'),\n",
    "         ('DDB Needham', 'IN', 'New York'),\n",
    "         ('Kaplan Thaler Group', 'IN', 'New York'),\n",
    "         ('BBDO South', 'IN', 'Atlanta'),\n",
    "         ('Georgia-Pacific', 'IN', 'Atlanta')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BBDO South', 'Georgia-Pacific']\n"
     ]
    }
   ],
   "source": [
    "query = [e1 for (e1, rel, e2) in locs if e2=='Atlanta']\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next sections of this notebook, we'll follow a process of processing raw text to convert it into a more structured form so that information extraction is possible.\n",
    "\n",
    "The first three steps of this task are segmentation, tokenization, and part-of-speech tagging. \n",
    "\n",
    "Let's define a function to do those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Chunking is a technique for entity detection, which segments and labels multi-token sequences.\n",
    "\n",
    "### Noun Phrase Chunking\n",
    "\n",
    "In Noun Phrase chunking (NP chunking), we search for chucks corresponding to individual noun phrases. \n",
    "\n",
    "Let's construct a simple NP-chunker by defining a chunk grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
