{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to chapter 10, this chapter is also text/idea heavy. It focuses on the maintenance of linguistic data, or corpora. \n",
    "\n",
    "While this notebook includes code cells for examples of data format conversions, data extraction, and other tasks involved in managing linguistic data, it is recommended to go through the chapter in the book to gain a better understanding of the major ideas involved.\n",
    "\n",
    "https://www.nltk.org/book/ch11.html\n",
    "\n",
    "# Managing Linguistic Data\n",
    "\n",
    "## Corpus Structure: A Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a sample from the TIMIT corpus. You can access its documentation in the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids() to see a list of the 160 recorded utterances in the corpus sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dr1-fvmh0/sa1.phn',\n",
       " 'dr1-fvmh0/sa1.txt',\n",
       " 'dr1-fvmh0/sa1.wav',\n",
       " 'dr1-fvmh0/sa1.wrd',\n",
       " 'dr1-fvmh0/sa2.phn',\n",
       " 'dr1-fvmh0/sa2.txt',\n",
       " 'dr1-fvmh0/sa2.wav',\n",
       " 'dr1-fvmh0/sa2.wrd',\n",
       " 'dr1-fvmh0/si1466.phn',\n",
       " 'dr1-fvmh0/si1466.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.fileids()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item has a phonetic transcription which can be accessed using the phones() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl', 's', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']\n"
     ]
    }
   ],
   "source": [
    "phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n",
    "print(phonetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 7812, 10610),\n",
       " ('had', 10610, 14496),\n",
       " ('your', 14496, 15791),\n",
       " ('dark', 15791, 20720),\n",
       " ('suit', 20720, 25647),\n",
       " ('in', 25647, 26906),\n",
       " ('greasy', 26906, 32668),\n",
       " ('wash', 32668, 37890),\n",
       " ('water', 38531, 42417),\n",
       " ('all', 43091, 46052),\n",
       " ('year', 46052, 50522)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.word_times('dr1-fvmh0/sa1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this text data, TIMIT includes a lexicon that provides the canonical pronunciation of every word, which can be compared with a particular utterance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timitdict = nltk.corpus.timit.transcription_dict()\n",
    "timitdict['greasy'] + timitdict['wash'] + timitdict['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic[17:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86', birthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS', comments='BEST NEW ENGLAND ACCENT SO FAR')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing demographic data of a particular ite\n",
    "nltk.corpus.timit.spkrinfo('dr1-fvmh0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Acquiring Data\n",
    "  \n",
    "  ### Obtaining Data from the Word Processor Files\n",
    "  \n",
    "Consider the following fragment of a lexical entry: \"sleep [sli:p] v.i. condition of body and mind...\". We can enter this in MSWord, then \"Save as Web Page\", then inspect the resulting HTML file:\n",
    "\n",
    "```\n",
    "<p class=MsoNormal>sleep\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  [<span class=SpellE>sli:p</span>]\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <b><span style='font-size:11.0pt'>v.i.</span></b>\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <i>a condition of body and mind ...<o:p></o:p></i>\n",
    "</p>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p class=MsoNormal>sleep\\n  <span style='mso-spacerun:yes'> </span>\\n  [<span class=SpellE>sli:p</span>]\\n  <span style='mso-spacerun:yes'> </span>\\n  <b><span style='font-size:11.0pt'>v.i.</span></b>\\n  <span style='mso-spacerun:yes'> </span>\\n  <i>a condition of body and mind ...<o:p></o:p></i>\\n</p>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n",
    "pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n",
    "document = open(\"data/dict.htm\", encoding=\"windows-1252\").read()\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the used pos tags as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_pos = set(re.findall(pattern, document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v.i.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the data is correctly formatted, we can write other programs to convert the data into a different format.\n",
    "\n",
    "### Obtaining Data from Spreadsheets and Databases\n",
    "\n",
    "Our program might perform a linguistically motivated query which cannot be expressed in SQL, e.g. select all words that appear in example sentences for which no dictionary entry is provided. For this task, we would need to extract enough information from a record for it to be uniquely identified, along with the headwords and example sentences. Let's suppose this information was now available in a CSV file dict.csv:\n",
    "\n",
    "```\n",
    "\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n",
    "\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n",
    "\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\n",
    "```\n",
    "\n",
    "We can express this query in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = csv.reader(open('data/dict.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'each', 'down', 'mind', 'and', 'body', 'setting', 'cease', 'lifting', 'condition', 'foot', 'by', '...', 'to', 'progress', 'sleep', 'a', 'of'}\n"
     ]
    }
   ],
   "source": [
    "lexemes, defns = zip(*pairs)\n",
    "defn_words = set(w for defn in defns for w in defn.split())\n",
    "print(defn_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each', 'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(defn_words.difference(lexemes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to enrich the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data Formats\n",
    "\n",
    "Consider a case where we had to have a build an inverted index using some input data. \n",
    "\n",
    "We can continue the use of the data from the above example, and consider an example where we have to construct an index that maps the words of a dictionary definition to the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = nltk.Index((defn_word, lexeme) \n",
    "                 for (lexeme, defn) in pairs \n",
    "                 for defn_word in nltk.word_tokenize(defn) \n",
    "                 if len(defn_word) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(list,\n",
       "      {'condition': ['sleep'],\n",
       "       'body': ['sleep'],\n",
       "       'mind': ['sleep'],\n",
       "       'progress': ['walk'],\n",
       "       'lifting': ['walk'],\n",
       "       'setting': ['walk'],\n",
       "       'down': ['walk'],\n",
       "       'each': ['walk'],\n",
       "       'foot': ['walk'],\n",
       "       'cease': ['wake'],\n",
       "       'sleep': ['wake']})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/dict.idx\", \"w\") as idx_file:\n",
    "     for word in sorted(idx):\n",
    "            idx_words = ', '.join(idx[word])\n",
    "            idx_line = \"{}: {}\".format(word, idx_words)\n",
    "            print(idx_line, file=idx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup by Pronunciation Similarity\n",
    "\n",
    "First, we identify confusible letter sequences and map complex versions to simpler versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),\n",
    "             ('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(word):\n",
    "    for patt, repl in mappings: \n",
    "        word = re.sub(patt, repl, word)\n",
    "    pieces = re.findall(\"[^aeiou]+\", word)\n",
    "    return \"\".join(char for piece in pieces for char in sorted(piece))[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lfnt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"illefent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lfnt'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"elephant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bskws'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"ebsekwieous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nclr'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"nuculerr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a mapping from signatures to words, for all words in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']\n"
     ]
    }
   ],
   "source": [
    "print(signatures[signature(\"nuclerr\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rank the results in terms of similarity to the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(word, wordlist):\n",
    "    ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)\n",
    "    return [word for (_, word) in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_spell(word):\n",
    "    sig = signature(word)\n",
    "    if sig in signatures:\n",
    "        return rank(word, signatures[sig])\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['olefiant', 'elephant', 'oliphant', 'elephanta']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_spell(\"illefent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nuclear',\n",
       " 'nucellar',\n",
       " 'anicular',\n",
       " 'inocular',\n",
       " 'unocular',\n",
       " 'unicolor',\n",
       " 'uniocular']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_spell(\"nuclar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_spell(\"eple\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, a simple program can facilitate access to lexical data in a context where the writing system of a language may not be standardized, or where users of the language may not have a good command of spellings.\n",
    "\n",
    "## Working with XML\n",
    "\n",
    "### The ElementTree Interface\n",
    "\n",
    "Let's illustrate the use of ElementTree using a collection of Shakespeare plays that have been formatted using XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the xml file\n",
    "merchant_file = nltk.data.find(\"corpora/shakespeare/merchant.xml\")\n",
    "raw = open(merchant_file).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\n",
      "<?xml-stylesheet type=\"text/css\" href=\"shakes.css\"?>\n",
      "<!-- <!DOCTYPE PLAY SYSTEM \"play.dtd\"> -->\n",
      "\n",
      "<PLAY>\n",
      "<TITLE>The Merchant of Venice</TITLE>\n"
     ]
    }
   ],
   "source": [
    "# Inspecting contents\n",
    "print(raw[:163])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TITLE>ACT I</TITLE>\n",
      "\n",
      "<SCENE><TITLE>SCENE I.  Venice. A street.</TITLE>\n",
      "<STAGEDIR>Enter ANTONIO, SALARINO, and SALANIO</STAGEDIR>\n",
      "\n",
      "<SPEECH>\n",
      "<SPEAKER>ANTONIO</SPEAKER>\n",
      "<LINE>In sooth, I know not why I am so sad:</LINE>\n"
     ]
    }
   ],
   "source": [
    "print(raw[1789:2006])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we have accessed the XML data as string. The next step is to process the file contents as structured XML data using `ElementTree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import ElementTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'PLAY' at 0x1a175d3c50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant = ElementTree().parse(merchant_file)\n",
    "merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'TITLE' at 0x1a219b5650>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Merchant of Venice'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sajal/anaconda/envs/general/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Element 'TITLE' at 0x1a219b5650>,\n",
       " <Element 'PERSONAE' at 0x1a219b56b0>,\n",
       " <Element 'SCNDESCR' at 0x1a219b9350>,\n",
       " <Element 'PLAYSUBT' at 0x1a219b93b0>,\n",
       " <Element 'ACT' at 0x1a219b9410>,\n",
       " <Element 'ACT' at 0x1a219e0dd0>,\n",
       " <Element 'ACT' at 0x1a21a16230>,\n",
       " <Element 'ACT' at 0x1a21a4c230>,\n",
       " <Element 'ACT' at 0x1a21a73e90>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant.getchildren()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The play consists of a title, the personae, a scene description, a subtitle, and five acts. Each act has a title and some scenes, and each scene consists of speeches which are made up of lines, a structure with four levels of nesting. Let's dig down into Act IV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACT IV'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'SCENE' at 0x1a21a4c2f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE I.  Venice. A court of justice.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'SPEECH' at 0x1a21a59d70>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'SPEAKER' at 0x1a21a59dd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][54][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PORTIA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][54][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'LINE' at 0x1a21a59e30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][54][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The quality of mercy is not strain'd,\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant[-2][1][54][1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the types we are interested in (ex: ACTS) using `merchant.findall('ACT')`. Below is an example of doing tag-specific searches at every level of nesting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;\n",
      "Act 3 Scene 2 Speech 9: Fading in music: that the comparison\n",
      "Act 3 Scene 2 Speech 9: And what is music then? Then music is\n",
      "Act 5 Scene 1 Speech 23: And bring your music forth into the air.\n",
      "Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music\n",
      "Act 5 Scene 1 Speech 23: And draw her home with music.\n",
      "Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.\n",
      "Act 5 Scene 1 Speech 25: Or any air of music touch their ears,\n",
      "Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet\n",
      "Act 5 Scene 1 Speech 25: But music for the time doth change his nature.\n",
      "Act 5 Scene 1 Speech 25: The man that hath no music in himself,\n",
      "Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.\n",
      "Act 5 Scene 1 Speech 29: It is your music, madam, of the house.\n",
      "Act 5 Scene 1 Speech 32: No better a musician than the wren.\n"
     ]
    }
   ],
   "source": [
    "for i, act in enumerate(merchant.findall('ACT')):\n",
    "    for j, scene in enumerate(act.findall('SCENE')):\n",
    "        for k, speech in enumerate(scene.findall('SPEECH')):\n",
    "            for line in speech.findall('LINE'):\n",
    "                if 'music' in str(line.text):\n",
    "                    print(\"Act %d Scene %d Speech %d: %s\" % (i+1, j+1, k+1, line.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of explicitly navigating down the hierarchy, we can also search for embedded elements. \n",
    "\n",
    "The example below examines the sequence of speakers to build a frequency distribution for speakers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "speaker_seq = [s.text for s in merchant.findall(\"ACT/SCENE/SPEECH/SPEAKER\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANTONIO', 'SALARINO', 'SALANIO', 'SALARINO', 'ANTONIO']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PORTIA', 117),\n",
       " ('SHYLOCK', 79),\n",
       " ('BASSANIO', 73),\n",
       " ('GRATIANO', 48),\n",
       " ('ANTONIO', 47)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_freq = Counter(speaker_seq)\n",
    "top5 = speaker_freq.most_common(5)\n",
    "top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look for patterns in which speakers follows who in the dialogues. We'll only do this for the top 5 speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "abbreviate = defaultdict(lambda: 'OTH') # OTH = Other than top 5\n",
    "for speaker, _ in top5:\n",
    "    abbreviate[speaker] = speaker[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ANTO BASS GRAT  OTH PORT SHYL \n",
      "ANTO    0   11    4   11    9   12 \n",
      "BASS   10    0   11   10   26   16 \n",
      "GRAT    6    8    0   19    9    5 \n",
      " OTH    8   16   18  153   52   25 \n",
      "PORT    7   23   13   53    0   21 \n",
      "SHYL   15   15    2   26   21    0 \n"
     ]
    }
   ],
   "source": [
    "speaker_seq2 = [abbreviate[speaker] for speaker in speaker_seq]\n",
    "cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ElementTree for Accessing Toolbox Data\n",
    "\n",
    "We can use the toolbox.xml() method to access a Toolbox file and load it into an elementtree object. This file contains a lexicon for the Rotokas language of Papua New Guinea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to access the contents of the lexicon object, by indexes and by paths. Indexes use the familiar syntax, thus lexicon[3] returns entry number 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element 'lx' at 0x1a176cddd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lx'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[3][0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kaa'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon[3][0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way to access the contents of the lexicon object uses paths. The lexicon is a series of record objects, each containing a series of field objects, such as lx and ps. We can conveniently address all of the lexemes using the path record/lx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaa',\n",
       " 'kaa',\n",
       " 'kaa',\n",
       " 'kaakaaro',\n",
       " 'kaakaaviko',\n",
       " 'kaakaavo',\n",
       " 'kaakaoko',\n",
       " 'kaakasi',\n",
       " 'kaakau',\n",
       " 'kaakauko']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lexeme.text.lower() for lexeme in lexicon.findall('record/lx')][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the Toolbox data in XML format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<record>\n",
      "    <lx>kaa</lx>\n",
      "    <ps>N</ps>\n",
      "    <pt>MASC</pt>\n",
      "    <cl>isi</cl>\n",
      "    <ge>cooking banana</ge>\n",
      "    <tkp>banana bilong kukim</tkp>\n",
      "    <pt>itoo</pt>\n",
      "    <sf>FLORA</sf>\n",
      "    <dt>12/Aug/2005</dt>\n",
      "    <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>\n",
      "    <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>\n",
      "    <xe>Taeavi planted banana in order to cook it.</xe>\n",
      "  </record>"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from nltk.util import elementtree_indent\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "elementtree_indent(lexicon)\n",
    "tree = ElementTree(lexicon[3])\n",
    "tree.write(sys.stdout, encoding=\"unicode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Toolbox Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import toolbox\n",
    "lexicon = toolbox.xml('rotokas.dic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the average number of fields for each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.635955056179775"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(entry) for entry in lexicon) / len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will discuss two tasks that arise in the context of documentary linguistics, neither of which is supported by the Toolbox software.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Field to Each Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convenient to add new fields that are derived automatically from existing ones. Such fields often facilitate search and analysis. \n",
    "\n",
    "For instance below, we define a function cv() which maps a string of consonants and vowels to the corresponding CV sequence, e.g. kakapua would map to CVCVCVV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import SubElement\n",
    "\n",
    "def cv(s):\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^a-z]',     r'_', s)\n",
    "    s = re.sub(r'[aeiou]',    r'V', s)\n",
    "    s = re.sub(r'[^V_]',      r'C', s)\n",
    "    return (s)\n",
    "\n",
    "def add_cv_field(entry):\n",
    "    for field in entry:\n",
    "        if field.tag == 'lx':\n",
    "            cv_field = SubElement(entry, 'cv')\n",
    "            cv_field.text = cv(field.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')\n",
    "add_cv_field(lexicon[53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\lx kaeviro\n",
      "\\ps V\n",
      "\\pt A\n",
      "\\ge lift off\n",
      "\\ge take off\n",
      "\\tkp go antap\n",
      "\\sc MOTION\n",
      "\\vx 1\n",
      "\\nt used to describe action of plane\n",
      "\\dt 03/Jun/2005\n",
      "\\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.\n",
      "\\xp Pita i go antap na lukim haus win i bagarapim.\n",
      "\\xe Peter went to look at the house that the wind destroyed.\n",
      "\\cv CVVCVCV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nltk.toolbox.to_sfm_string(lexicon[53]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating a Toolbox Lexicon\n",
    "\n",
    "Many lexicons in Toolbox format do not conform to any particular schema. Some entries may include extra fields, or may order existing fields in a new way. Instead of manually inspecting thousands of lexical entries, we can identify frequent field sequences with the help of a `Counter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "field_sequences = Counter(':'.join(field.tag for field in entry) for entry in lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),\n",
       " ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20),\n",
       " ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe', 17)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_sequences.most_common()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use a grammar to validate entries when we iterate over them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring('''\n",
    "  S -> Head PS Glosses Comment Date Sem_Field Examples\n",
    "  Head -> Lexeme Root\n",
    "  Lexeme -> \"lx\"\n",
    "  Root -> \"rt\" |\n",
    "  PS -> \"ps\"\n",
    "  Glosses -> Gloss Glosses |\n",
    "  Gloss -> \"ge\" | \"tkp\" | \"eng\"\n",
    "  Date -> \"dt\"\n",
    "  Sem_Field -> \"sf\"\n",
    "  Examples -> Example Ex_Pidgin Ex_English Examples |\n",
    "  Example -> \"ex\"\n",
    "  Ex_Pidgin -> \"xp\"\n",
    "  Ex_English -> \"xe\"\n",
    "  Comment -> \"cmt\" | \"nt\" |\n",
    "  ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_lexicon(grammar, lexicon, ignored_tags):\n",
    "    rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "    for entry in lexicon:\n",
    "        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]\n",
    "        if list(rd_parser.parse(marker_list)):\n",
    "            print(\"+\", ':'.join(marker_list))\n",
    "        else:\n",
    "            print(\"-\", ':'.join(marker_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = toolbox.xml('rotokas.dic')[10:20]\n",
    "ignored_tags = ['arg', 'dcsv', 'pt', 'vx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:tkp:nt:sf:dt\n",
      "- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe\n",
      "- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe\n",
      "- lx:rt:ps:ge:ge:tkp:dt\n",
      "- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe\n",
      "- lx:rt:ps:ge:tkp:dt:ex:xp:xe\n",
      "- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe\n"
     ]
    }
   ],
   "source": [
    "validate_lexicon(grammar, lexicon, ignored_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading: https://www.nltk.org/book/ch11.html#ref-ignored-tags"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
