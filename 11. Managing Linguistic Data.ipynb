{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to chapter 10, this chapter is also text/idea heavy. It focuses on the maintenance of linguistic data, or corpora. \n",
    "\n",
    "While this notebook includes code cells for examples of data format conversions, data extraction, and other tasks involved in managing linguistic data, it is recommended to go through the chapter in the book to gain a better understanding of the major ideas involved.\n",
    "\n",
    "https://www.nltk.org/book/ch11.html\n",
    "\n",
    "# Managing Linguistic Data\n",
    "\n",
    "## Corpus Structure: A Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK includes a sample from the TIMIT corpus. You can access its documentation in the usual way, using help(nltk.corpus.timit). Print nltk.corpus.timit.fileids() to see a list of the 160 recorded utterances in the corpus sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dr1-fvmh0/sa1.phn',\n",
       " 'dr1-fvmh0/sa1.txt',\n",
       " 'dr1-fvmh0/sa1.wav',\n",
       " 'dr1-fvmh0/sa1.wrd',\n",
       " 'dr1-fvmh0/sa2.phn',\n",
       " 'dr1-fvmh0/sa2.txt',\n",
       " 'dr1-fvmh0/sa2.wav',\n",
       " 'dr1-fvmh0/sa2.wrd',\n",
       " 'dr1-fvmh0/si1466.phn',\n",
       " 'dr1-fvmh0/si1466.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.fileids()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each item has a phonetic transcription which can be accessed using the phones() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl', 's', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']\n"
     ]
    }
   ],
   "source": [
    "phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')\n",
    "print(phonetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 7812, 10610),\n",
       " ('had', 10610, 14496),\n",
       " ('your', 14496, 15791),\n",
       " ('dark', 15791, 20720),\n",
       " ('suit', 20720, 25647),\n",
       " ('in', 25647, 26906),\n",
       " ('greasy', 26906, 32668),\n",
       " ('wash', 32668, 37890),\n",
       " ('water', 38531, 42417),\n",
       " ('all', 43091, 46052),\n",
       " ('year', 46052, 50522)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.timit.word_times('dr1-fvmh0/sa1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this text data, TIMIT includes a lexicon that provides the canonical pronunciation of every word, which can be compared with a particular utterance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timitdict = nltk.corpus.timit.transcription_dict()\n",
    "timitdict['greasy'] + timitdict['wash'] + timitdict['water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonetic[17:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86', birthdate='01/08/60', ht='5\\'05\"', race='WHT', edu='BS', comments='BEST NEW ENGLAND ACCENT SO FAR')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing demographic data of a particular ite\n",
    "nltk.corpus.timit.spkrinfo('dr1-fvmh0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Acquiring Data\n",
    "  \n",
    "  ### Obtaining Data from the Word Processor Files\n",
    "  \n",
    "Consider the following fragment of a lexical entry: \"sleep [sli:p] v.i. condition of body and mind...\". We can enter this in MSWord, then \"Save as Web Page\", then inspect the resulting HTML file:\n",
    "\n",
    "```\n",
    "<p class=MsoNormal>sleep\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  [<span class=SpellE>sli:p</span>]\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <b><span style='font-size:11.0pt'>v.i.</span></b>\n",
    "  <span style='mso-spacerun:yes'> </span>\n",
    "  <i>a condition of body and mind ...<o:p></o:p></i>\n",
    "</p>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<p class=MsoNormal>sleep\\n  <span style='mso-spacerun:yes'> </span>\\n  [<span class=SpellE>sli:p</span>]\\n  <span style='mso-spacerun:yes'> </span>\\n  <b><span style='font-size:11.0pt'>v.i.</span></b>\\n  <span style='mso-spacerun:yes'> </span>\\n  <i>a condition of body and mind ...<o:p></o:p></i>\\n</p>\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])\n",
    "pattern = re.compile(r\"'font-size:11.0pt'>([a-z.]+)<\")\n",
    "document = open(\"data/dict.htm\", encoding=\"windows-1252\").read()\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the used pos tags as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_pos = set(re.findall(pattern, document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v.i.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "used_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the data is correctly formatted, we can write other programs to convert the data into a different format.\n",
    "\n",
    "### Obtaining Data from Spreadsheets and Databases\n",
    "\n",
    "Our program might perform a linguistically motivated query which cannot be expressed in SQL, e.g. select all words that appear in example sentences for which no dictionary entry is provided. For this task, we would need to extract enough information from a record for it to be uniquely identified, along with the headwords and example sentences. Let's suppose this information was now available in a CSV file dict.csv:\n",
    "\n",
    "```\n",
    "\"sleep\",\"sli:p\",\"v.i\",\"a condition of body and mind ...\"\n",
    "\"walk\",\"wo:k\",\"v.intr\",\"progress by lifting and setting down each foot ...\"\n",
    "\"wake\",\"weik\",\"intrans\",\"cease to sleep\"\n",
    "```\n",
    "\n",
    "We can express this query in code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = csv.reader(open('data/dict.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'each', 'down', 'mind', 'and', 'body', 'setting', 'cease', 'lifting', 'condition', 'foot', 'by', '...', 'to', 'progress', 'sleep', 'a', 'of'}\n"
     ]
    }
   ],
   "source": [
    "lexemes, defns = zip(*pairs)\n",
    "defn_words = set(w for defn in defns for w in defn.split())\n",
    "print(defn_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each', 'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(defn_words.difference(lexemes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to enrich the lexicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data Formats\n",
    "\n",
    "Consider a case where we had to have a build an inverted index using some input data. \n",
    "\n",
    "We can continue the use of the data from the above example, and consider an example where we have to construct an index that maps the words of a dictionary definition to the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = nltk.Index((defn_word, lexeme) \n",
    "                 for (lexeme, defn) in pairs \n",
    "                 for defn_word in nltk.word_tokenize(defn) \n",
    "                 if len(defn_word) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(list,\n",
       "      {'condition': ['sleep'],\n",
       "       'body': ['sleep'],\n",
       "       'mind': ['sleep'],\n",
       "       'progress': ['walk'],\n",
       "       'lifting': ['walk'],\n",
       "       'setting': ['walk'],\n",
       "       'down': ['walk'],\n",
       "       'each': ['walk'],\n",
       "       'foot': ['walk'],\n",
       "       'cease': ['wake'],\n",
       "       'sleep': ['wake']})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/dict.idx\", \"w\") as idx_file:\n",
    "     for word in sorted(idx):\n",
    "            idx_words = ', '.join(idx[word])\n",
    "            idx_line = \"{}: {}\".format(word, idx_words)\n",
    "            print(idx_line, file=idx_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup by Pronunciation Similarity\n",
    "\n",
    "First, we identify confusible letter sequences and map complex versions to simpler versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),\n",
    "             ('[aeiou]+', 'a'), (r'(.)\\1', r'\\1')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(word):\n",
    "    for patt, repl in mappings: \n",
    "        word = re.sub(patt, repl, word)\n",
    "    pieces = re.findall(\"[^aeiou]+\", word)\n",
    "    return \"\".join(char for piece in pieces for char in sorted(piece))[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lfnt'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"illefent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lfnt'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"elephant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bskws'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"ebsekwieous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nclr'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signature(\"nuculerr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a mapping from signatures to words, for all words in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']\n"
     ]
    }
   ],
   "source": [
    "print(signatures[signature(\"nuclerr\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we rank the results in terms of similarity to the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(word, wordlist):\n",
    "    ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)\n",
    "    return [word for (_, word) in ranked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_spell(word):\n",
    "    sig = signature(word)\n",
    "    if sig in signatures:\n",
    "        return rank(word, signatures[sig])\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_spell(\"illefent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
